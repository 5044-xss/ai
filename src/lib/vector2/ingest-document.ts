// src/lib/vector-store.ts
import * as fs from 'fs';
import * as path from 'path';

// å®šä¹‰å­˜å‚¨ç»“æ„
export interface StoredChunk {
  text: string;
  embedding: number[]; // å‘é‡
  metadata: {
    source: string;
    htmlPath: string;
    startIdx: number;
    paragraphId?: string;
  };
}

const VECTOR_STORE_PATH = path.join(process.cwd(), 'data', 'vectors.json');

// ç¡®ä¿ data ç›®å½•å­˜åœ¨
if (!fs.existsSync(path.dirname(VECTOR_STORE_PATH))) {
  fs.mkdirSync(path.dirname(VECTOR_STORE_PATH), { recursive: true });
}

// ç”Ÿæˆ embeddingï¼ˆä½¿ç”¨ BGE æ¨¡å‹ï¼‰
export async function generateEmbedding(text: string): Promise<number[]> {
  const { pipeline } = await import('@xenova/transformers');
  // const embedder = await pipeline('feature-extraction', 'Xenova/bge-small-zh-v1.5', {
  //   quantized: true, // æ›´å¿«ã€æ›´çœå†…å­˜
  // });
  const embedder = await pipeline(
        'feature-extraction',
        '../../../../models/bge-small-zh-v1.5', // â† ä½ çš„æœ¬åœ°è·¯å¾„
        {
          quantized: true, // å¿…é¡»ä¸º true
        }
      );

  console.log(text,'text---------');
  
  // BGE æœ€ä½³å®è·µï¼šåŠ  document å‰ç¼€
  const output = await embedder(`document: ${text}`, {
    pooling: 'mean',
    normalize: true,
  });
 
  const result = Array.from(output.data);
  return result
}

// ä¿å­˜ chunks åˆ°æœ¬åœ°å‘é‡åº“
export async function saveChunksToVectorStore(
  chunks: Array<{
    text: string;
    metadata: {
      source: string;
      htmlPath: string;
      startIdx: number;
      paragraphId?: string;
    };
  }>
): Promise<void> {
  console.log(`ğŸ“¦ å‡†å¤‡ä¸º ${chunks.length} ä¸ªç‰‡æ®µç”Ÿæˆå‘é‡...`);

  // 1. ä¸ºæ¯ä¸ª chunk ç”Ÿæˆ embedding
  const embeddedChunks: StoredChunk[] = [];
  for (const chunk of chunks) {
    const embedding = await generateEmbedding(chunk.text);
    embeddedChunks.push({
      text: chunk.text,
      embedding,
      metadata: chunk.metadata,
    });
  }

  // 2. è¯»å–ç°æœ‰å‘é‡åº“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
  let existingChunks: StoredChunk[] = [];
  if (fs.existsSync(VECTOR_STORE_PATH)) {
    const rawData = fs.readFileSync(VECTOR_STORE_PATH, 'utf-8');
    const data = JSON.parse(rawData);
    existingChunks = data.chunks || [];
  }

  // 3. å»é‡ï¼šç§»é™¤åŒåæ–‡æ¡£çš„æ—§ chunksï¼ˆé¿å…é‡å¤ï¼‰
  const newSources = new Set(chunks.map(c => c.metadata.source));
  const filteredExisting = existingChunks.filter(
    c => !newSources.has(c.metadata.source)
  );

  // 4. åˆå¹¶æ–°æ—§æ•°æ®
  const allChunks = [...filteredExisting, ...embeddedChunks];

  // 5. å†™å…¥æ–‡ä»¶
  fs.writeFileSync(
    VECTOR_STORE_PATH,
    JSON.stringify({ chunks: allChunks }, null, 2)
  );

  console.log(`âœ… å·²ä¿å­˜ ${embeddedChunks.length} ä¸ªæ–°ç‰‡æ®µåˆ° ${VECTOR_STORE_PATH}`);
}